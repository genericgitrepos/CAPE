{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9cf67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_shape: tuple, num_classes: int) -> nn.Module:\n",
    "    c, h, w = input_shape\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(c, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64 * (h // 4) * (w // 4), 256), nn.ReLU(),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probe_features(model, X, y, criterion):\n",
    "    model.to(DEVICE).train()\n",
    "    logP = np.log(sum(p.numel() for p in model.parameters()))\n",
    "    logB = np.log(min(32, X.size(0)))\n",
    "    Xp, yp = X[:32].to(DEVICE), y[:32].to(DEVICE)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    g2_list, tau_list = [], []\n",
    "    for xi, yi in zip(Xp, yp):\n",
    "        xi, yi = xi.unsqueeze(0), yi.unsqueeze(0)\n",
    "        model.zero_grad()\n",
    "        logits = model(xi)\n",
    "        loss   = criterion(logits, yi)\n",
    "        grads  = torch.autograd.grad(loss, params, retain_graph=True)\n",
    "        gv     = torch.cat([g.contiguous().view(-1) for g in grads])\n",
    "        g2_list.append((gv**2).sum().item())\n",
    "        model.zero_grad()\n",
    "        true_logit = logits.view(-1)[yi.item()]\n",
    "        grads_f    = torch.autograd.grad(true_logit, params, retain_graph=True)\n",
    "        fv         = torch.cat([g.contiguous().view(-1) for g in grads_f])\n",
    "        tau_list.append((fv**2).sum().item())\n",
    "    logG2  = np.log(np.mean(g2_list))\n",
    "    logTau = np.log(np.sum(tau_list))\n",
    "    return np.array([logP, logB, logG2, logTau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_convergence(model, X, y, eps, lr, criterion):\n",
    "    model.to(DEVICE).train()\n",
    "    X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    init_loss = None\n",
    "    for t in range(1, 501):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss   = criterion(logits, y)\n",
    "        if t == 1:\n",
    "            init_loss = loss.item()\n",
    "        if loss.item() <= eps * init_loss:\n",
    "            return t\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATASETS = {\n",
    "    'MNIST':        (datasets.MNIST,      {'train': True}),\n",
    "    'FashionMNIST': (datasets.FashionMNIST,{'train': True}),\n",
    "    'CIFAR10':      (datasets.CIFAR10,    {'train': True}),\n",
    "    'CIFAR100':     (datasets.CIFAR100,   {'train': True})\n",
    "}\n",
    "TRANSFORMS = {name: transforms.Compose([transforms.ToTensor()]) for name in DATASETS}\n",
    "\n",
    "LR_VALUES     = [0.0005, 0.001, 0.005]\n",
    "BATCH_SIZES   = [50, 100]\n",
    "EPS_VALUES    = [0.1, 0.15, 0.2]\n",
    "N_EVAL_TRIALS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb6cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv('../meta_datasets/meta_dataset_cnn.csv')\n",
    "FEATURES = ['logP','logB','logG2','logTau','logLR','logN']\n",
    "X_meta  = df_meta[FEATURES].values\n",
    "y_meta  = df_meta['T_star'].values\n",
    "meta_reg = XGBRegressor(n_estimators=200, max_depth=4, random_state=42)\n",
    "meta_reg.fit(X_meta, y_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7889929",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CAPE CNN Evaluation:\")\n",
    "records = []\n",
    "\n",
    "for ds_name, (ds_cls, ds_args) in DATASETS.items():\n",
    "    ds = ds_cls(root='./data', download=True,\n",
    "                transform=TRANSFORMS[ds_name], **ds_args)\n",
    "    num_classes = len(ds.classes)\n",
    "    input_shape = ds[0][0].shape\n",
    "    total_N     = len(ds)\n",
    "    criterion   = nn.CrossEntropyLoss()\n",
    "\n",
    "    for lr in LR_VALUES:\n",
    "        logLR = np.log(lr)\n",
    "        for B in BATCH_SIZES:\n",
    "            loader = DataLoader(ds, batch_size=B, shuffle=True)\n",
    "            for eps in EPS_VALUES:\n",
    "                y_preds, y_trues = [], []\n",
    "                for _ in range(N_EVAL_TRIALS):\n",
    "                    model = build_cnn(input_shape, num_classes)\n",
    "                    Xp, yp = next(iter(loader))\n",
    "                    z0 = extract_probe_features(model, Xp, yp, criterion)\n",
    "                    z  = np.concatenate([z0, [logLR, np.log(total_N)]])\n",
    "                    T_pred = meta_reg.predict(z.reshape(1, -1))[0]\n",
    "                    T_act  = measure_convergence(model, Xp, yp, eps, lr, criterion)\n",
    "                    y_preds.append(T_pred)\n",
    "                    y_trues.append(T_act)\n",
    "\n",
    "                mae  = mean_absolute_error(y_trues, y_preds)\n",
    "                corr = np.corrcoef(y_trues, y_preds)[0, 1]\n",
    "                print(f\"{ds_name} | lr={lr} | B={B} | eps={eps} | TT error={mae:.0f} | Corr={corr:.2f}\")\n",
    "                records.append({\n",
    "                    'dataset':    ds_name,\n",
    "                    'TT_error':   mae,\n",
    "                    'Corr':       corr\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(records)\n",
    "df_summary = df_results.groupby(\"dataset\").agg({\n",
    "    \"TT_error\": \"mean\",\n",
    "    \"Corr\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "df_summary[\"TT_error\"] = df_summary[\"TT_error\"].round(2)\n",
    "df_summary[\"Corr\"] = df_summary[\"Corr\"].round(3)\n",
    "\n",
    "# --- Save results ---\n",
    "df_summary.to_csv(\"CNN_evaluation_dataset_avg.csv\", index=False)\n",
    "print(\"Per-dataset average results saved to CNN_evaluation_dataset_avg.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
