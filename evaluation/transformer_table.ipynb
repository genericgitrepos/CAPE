{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11566eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e619173",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ARCHITECTURE = 'deit_tiny_patch16_224'\n",
    "LR_VALUES = [0.0005, 0.001, 0.002]\n",
    "BATCH_SIZES = [50, 100]\n",
    "EPS_VALUES = [0.6, 0.5, 0.4]\n",
    "N_EVAL_TRIALS = 10\n",
    "MAX_STEPS = 500\n",
    "\n",
    "# --- Datasets for Meta-Evaluation ---\n",
    "DATASETS = {\n",
    "    'MNIST':         (datasets.MNIST, {'train': True}),\n",
    "    'FashionMNIST':  (datasets.FashionMNIST, {'train': True}),\n",
    "    'CIFAR10':       (datasets.CIFAR10, {'train': True}),\n",
    "    'QMNIST':        (datasets.QMNIST, {'train': True, 'what': 'train'}),\n",
    "}\n",
    "\n",
    "# Resize + RGB conversion for all datasets\n",
    "TRANSFORMS = {}\n",
    "for name in DATASETS:\n",
    "    if name == 'CIFAR10' or name == 'CIFAR100':\n",
    "        mean = [0.4914, 0.4822, 0.4465]\n",
    "        std = [0.2470, 0.2435, 0.2616]\n",
    "    else:\n",
    "        mean = [0.1307] * 3\n",
    "        std = [0.3081] * 3\n",
    "    TRANSFORMS[name] = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timm_model(name, num_classes):\n",
    "    model = timm.create_model(name, pretrained=True, num_classes=num_classes)\n",
    "    for pname, p in model.named_parameters():\n",
    "        if \"head\" not in pname and \"fc\" not in pname and \"classifier\" not in pname:\n",
    "            p.requires_grad = False\n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb86fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probe_features(model, X, y, criterion):\n",
    "    model.train()\n",
    "    logP = np.log(sum(p.numel() for p in model.parameters()))\n",
    "    logB = np.log(min(32, X.size(0)))\n",
    "    Xp, yp = X[:32].to(DEVICE), y[:32].to(DEVICE)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    g2_list, tau_list = [], []\n",
    "    for xi, yi in zip(Xp, yp):\n",
    "        xi, yi = xi.unsqueeze(0), yi.unsqueeze(0)\n",
    "        model.zero_grad()\n",
    "        logits = model(xi)\n",
    "        loss = criterion(logits, yi)\n",
    "        grads = torch.autograd.grad(loss, params, retain_graph=True)\n",
    "        gv = torch.cat([g.contiguous().view(-1) for g in grads])\n",
    "        g2_list.append((gv**2).sum().item())\n",
    "        model.zero_grad()\n",
    "        true_logit = logits.view(-1)[yi.item()]\n",
    "        grads_f = torch.autograd.grad(true_logit, params, retain_graph=True)\n",
    "        fv = torch.cat([g.contiguous().view(-1) for g in grads_f])\n",
    "        tau_list.append((fv**2).sum().item())\n",
    "    logG2 = np.log(np.mean(g2_list))\n",
    "    logTau = np.log(np.sum(tau_list))\n",
    "    return np.array([logP, logB, logG2, logTau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_convergence(model, loader, eps, lr, criterion):\n",
    "    model.train()\n",
    "    X0, y0 = next(iter(loader))\n",
    "    X0, y0 = X0.to(DEVICE), y0.to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        init_loss = criterion(model(X0), y0).item()\n",
    "    threshold = eps * init_loss\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    it = iter(loader)\n",
    "    for step in range(1, MAX_STEPS + 1):\n",
    "        try:\n",
    "            Xb, yb = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(loader)\n",
    "            Xb, yb = next(it)\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        if loss.item() <= threshold:\n",
    "            return step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return MAX_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv('../meta_datasets/meta_dataset_transformer.csv')\n",
    "FEATURES = ['logP','logB','logG2','logTau','logLR','logN']\n",
    "X_meta = df_meta[FEATURES].values\n",
    "y_meta = df_meta['T_star'].values\n",
    "meta_reg = XGBRegressor(n_estimators=200, max_depth=4, random_state=42)\n",
    "meta_reg.fit(X_meta, y_meta)\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"CAPE Transformer Evaluation:\")\n",
    "records = []\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04108088",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_name, (ds_cls, ds_args) in DATASETS.items():\n",
    "    print(f\"\\nEvaluating on: {ds_name}\")\n",
    "    full_ds = ds_cls(root='./data', download=True, transform=TRANSFORMS[ds_name], **ds_args)\n",
    "    subset = Subset(full_ds, range(1000))  # Keep evaluation fast\n",
    "    num_classes = len(full_ds.classes) if hasattr(full_ds, 'classes') else 10\n",
    "    total_N = len(subset)\n",
    "\n",
    "    for lr in LR_VALUES:\n",
    "        logLR = np.log(lr)\n",
    "        for B in BATCH_SIZES:\n",
    "            loader = DataLoader(subset, batch_size=B, shuffle=True)\n",
    "            for eps in EPS_VALUES:\n",
    "                y_preds, y_trues = [], []\n",
    "                for _ in range(N_EVAL_TRIALS):\n",
    "                    model = build_timm_model(ARCHITECTURE, num_classes)\n",
    "                    Xp, yp = next(iter(loader))\n",
    "                    z0 = extract_probe_features(model, Xp, yp, criterion)\n",
    "                    z = np.concatenate([z0, [logLR, np.log(total_N)]])\n",
    "                    T_pred = meta_reg.predict(z.reshape(1, -1))[0]\n",
    "                    T_act = measure_convergence(model, loader, eps, lr, criterion)\n",
    "                    y_preds.append(T_pred)\n",
    "                    y_trues.append(T_act)\n",
    "\n",
    "                mae = mean_absolute_error(y_trues, y_preds)\n",
    "                corr = np.corrcoef(y_trues, y_preds)[0, 1]\n",
    "                print(f\"{ds_name} | lr={lr} | B={B} | eps={eps} | MAE={mae:.1f} | Corr={corr:.2f}\")\n",
    "                records.append({\n",
    "                    'dataset': ds_name,\n",
    "                    'TT_error': mae,\n",
    "                    'Corr': corr\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57778557",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(records)\n",
    "df_summary = df_results.groupby(\"dataset\").agg({\n",
    "    \"TT_error\": \"mean\",\n",
    "    \"Corr\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "df_summary[\"TT_error\"] = df_summary[\"TT_error\"].round(2)\n",
    "df_summary[\"Corr\"] = df_summary[\"Corr\"].round(3)\n",
    "\n",
    "# --- Save to CSV ---\n",
    "df_summary.to_csv(\"Transformer_evaluation_dataset_avg.csv\", index=False)\n",
    "print(\"\\nSaved to Transformer_evaluation_dataset_avg.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
